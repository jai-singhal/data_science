{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T19:16:22.534547Z",
     "start_time": "2019-03-06T19:15:53.299213Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T19:27:23.679037Z",
     "start_time": "2019-03-06T19:16:40.969553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizing: word/sentence tokenizing\n",
    "\n",
    "lexicon and corparas\n",
    "\n",
    "corparas- body of text\n",
    "\n",
    "lexicon: words with the meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T19:28:26.055544Z",
     "start_time": "2019-03-06T19:28:25.885066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there!!', 'My name is Jai Singhal.', 'How are you?']\n",
      "['Hello', 'there', '!', '!', 'My', 'name', 'is', 'Jai', 'Singhal', '.', 'How', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "# tokenizing\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "example_text = \"Hello there!! My name is Jai Singhal. How are you?\"\n",
    "\n",
    "print(sent_tokenize(example_text))\n",
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T19:33:39.666876Z",
     "start_time": "2019-03-06T19:33:39.656907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself']\n"
     ]
    }
   ],
   "source": [
    "# stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "print(stop_words[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T19:36:41.211116Z",
     "start_time": "2019-03-06T19:36:41.203176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', '!', 'My', 'name', 'Jai', 'Singhal', '.', 'How', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hello there!! My name is Jai Singhal. How are you?\"\n",
    "words = word_tokenize(text)\n",
    "words = [word for word in words if word not in stop_words]\n",
    "print(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T19:36:53.838285Z",
     "start_time": "2019-03-06T19:36:53.830311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'My', 'name', 'Jai', 'Singhal', 'How']\n"
     ]
    }
   ],
   "source": [
    "# improving the words\n",
    "import re\n",
    "\n",
    "filtered_text = re.sub(r\"[^a-zA-Z]+\", ' ', text)\n",
    "words = word_tokenize(filtered_text)\n",
    "words = [word for word in words if word not in stop_words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T19:43:58.283147Z",
     "start_time": "2019-03-06T19:43:58.270996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "pythonista\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n",
      "python\n",
      "i am good in read\n",
      "i am good in read\n",
      "i am good in read\n"
     ]
    }
   ],
   "source": [
    "# steming\n",
    "\"\"\"\n",
    "riding == ride\n",
    "reading == read\n",
    "\"\"\"\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "sample_words = [\"python\", \"pythonista\", \n",
    "                \"pythoning\", \"pythoner\",\n",
    "                \"pythoned\", \"pythonly\", \n",
    "                \"pythoned\"]\n",
    "\n",
    "for w in sample_words:\n",
    "    print(ps.stem(w))\n",
    "    \n",
    "sentence1 = \"I am good in reading\"\n",
    "sentence2 = \"I am good in reader\"\n",
    "sentence3 = \"I am good in reads\"\n",
    "print(ps.stem(sentence1))\n",
    "print(ps.stem(sentence2))\n",
    "print(ps.stem(sentence3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorizing and Tagging Words\n",
    "\n",
    "- NNP proper noun, singular ‘Harrison’\n",
    "- NNPS proper noun, plural ‘Americans’\n",
    "- PDT predeterminer ‘all the kids’\n",
    "- POS possessive ending parent’s\n",
    "- PRP personal pronoun I, he, she\n",
    "- PRP$ possessive pronoun my, his, hers\n",
    "- RB adverb very, silently,\n",
    "- RBR adverb, comparative better\n",
    "- RBS adverb, superlative best\n",
    "- RP particle give up\n",
    "\n",
    "\n",
    "etc\n",
    "\n",
    "https://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T19:58:25.639602Z",
     "start_time": "2019-03-06T19:58:25.627555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Jai', 'NNP'), ('and', 'CC'), ('I', 'PRP'), ('love', 'VBP'), ('India', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "sentence = \"My name is Jai and I love India.\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(words)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T20:03:13.630726Z",
     "start_time": "2019-03-06T20:03:13.460265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Categorizing and Tagging Words\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "test_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(test_text)\n",
    "\n",
    "for i in tokenized[0:2]:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    print(tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
